{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:47:51.562225Z",
     "start_time": "2018-04-09T19:47:51.463382Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "class AdaBoostBinaryClassifier(object):\n",
    "    '''\n",
    "    INPUT:\n",
    "    - n_estimator (int)\n",
    "      * The number of estimators to use in boosting\n",
    "      * Default: 50\n",
    "\n",
    "    - learning_rate (float)\n",
    "      * Determines how fast the error would shrink\n",
    "      * Lower learning rate means more accurate decision boundary,\n",
    "        but slower to converge\n",
    "      * Default: 1\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1):\n",
    "\n",
    "        self.base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "        self.n_estimator = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Will be filled-in in the fit() step\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weight_ = np.zeros(self.n_estimator, dtype=np.float)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "        - x: 2d numpy array, feature matrix\n",
    "        - y: numpy array, labels\n",
    "\n",
    "        Build the estimators for the AdaBoost estimator.\n",
    "        '''\n",
    "\n",
    "        pass  ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    def _boost(self, x, y, sample_weight):\n",
    "        '''\n",
    "        INPUT:\n",
    "        - x: 2d numpy array, feature matrix\n",
    "        - y: numpy array, labels\n",
    "        - sample_weight: numpy array\n",
    "\n",
    "        OUTPUT:\n",
    "        - estimator: DecisionTreeClassifier\n",
    "        - sample_weight: numpy array (updated weights)\n",
    "        - estimator_weight: float (weight of estimator)\n",
    "\n",
    "        Go through one iteration of the AdaBoost algorithm. Build one estimator.\n",
    "        '''\n",
    "\n",
    "        estimator = clone(self.base_estimator)\n",
    "\n",
    "        ### YOUR CODE HERE ### \n",
    "#         (a) Fit \n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "        - x: 2d numpy array, feature matrix\n",
    "\n",
    "        OUTPUT:\n",
    "        - labels: numpy array of predictions (0 or 1)\n",
    "        '''\n",
    "\n",
    "        pass  ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    def score(self, x, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "        - x: 2d numpy array, feature matrix\n",
    "        - y: numpy array, labels\n",
    "\n",
    "        OUTPUT:\n",
    "        - score: float (accuracy score between 0 and 1)\n",
    "        '''\n",
    "\n",
    "        pass  ### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Include your code and answers in** `pair.py`.\n",
    "\n",
    "## Part 0: Intro to AdaBoost Classifier\n",
    "This morning we have encountered `AdaBoostRegressor` and its gradient\n",
    "descent variant, `GradientBoostingRegressor`. The base form of AdaBoost was\n",
    "introduced in 1995 as an ensemble classifier, `AdaBoostClassifier`.\n",
    "Understanding `AdaBoostClassifier` is regarded as the defacto\n",
    "introduction to the world of seemingly endless variants of boosting algorithms\n",
    "([refs](readings)). To gain a more entrenched understanding of boosting\n",
    "in general, I would recommend [this](readings/explaining_boosting.pdf).\n",
    "\n",
    "<br>\n",
    "\n",
    "Today we're going to implement the pseudocode below:\n",
    "\n",
    "![adaboost](images/adaboost_algorithm.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Implementing the AdaBoost Classifier\n",
    "Here we will build a simplified version of `AdaBoostClassifier`. In this case,\n",
    "our classifier, `AdaBoostBinaryClassifier`, will only predict binary outcomes.\n",
    "The starter code is in the [src](src) folder. The `boosting.py` file contains the\n",
    "core functions you would have to implement this afternoon. Fill in the rest depending\n",
    "on your progress. Remember it is more important to understand the intuition of the algorithm\n",
    "than get to the end of the exercise.\n",
    "\n",
    "<br>\n",
    "\n",
    "We're going to be using a spam dataset. It's in the [data](data) folder. You can see the feature names [here](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names).\n",
    "\n",
    "Here's how you should be able to run your code after you're finished:\n",
    "\n",
    "```python\n",
    "from boosting import AdaBoostBinaryClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__=='__main__':\n",
    "   data = np.genfromtxt('data/spam.csv', delimiter=',')\n",
    "\n",
    "   y = data[:, -1]\n",
    "   X = data[:, 0:-1]\n",
    "\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "   my_ada = AdaBoostBinaryClassifier(n_estimators=50)\n",
    "   my_ada.fit(X_train, y_train)\n",
    "   print \"Accuracy:\", my_ada.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "1. Take a look at the `__init__` method in `src/boosting.py`. You shouldn't need to change anything here. Note how we are creating Decision Trees that are just stumps! (max depth is 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Implement the `_boost` method. This will be doing steps (a)-(d) in the AdaBoost.M1 algorithm inside the for loop.\n",
    "\n",
    "    Because we need many copies of the estimator, the first step is to clone it. This code is given for you.\n",
    "\n",
    "    In this function `sample_weight` refers to the *wi*'s in the above description of the algorithm.\n",
    "\n",
    "    You will need to do these steps:\n",
    "\n",
    "    * Fix the Decision Tree using the weights. You can do this like this: `estimator.fit(X, y, sample_weight=sample_weight)`\n",
    "    * Calculate the error term (`estimator_error`)\n",
    "    * Calculate the alphas (`estimator_weight`)\n",
    "    * Update the weights (`sample_weight`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:43:53.871903Z",
     "start_time": "2018-04-09T19:43:53.618421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: None\n"
     ]
    }
   ],
   "source": [
    "from src.boosting import AdaBoostBinaryClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__=='__main__':\n",
    "   data = np.genfromtxt('data/spam.csv', delimiter=',')\n",
    "\n",
    "   y = data[:, -1]\n",
    "   X = data[:, 0:-1]\n",
    "\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "   my_ada = AdaBoostBinaryClassifier(n_estimators=50)\n",
    "   my_ada.fit(X_train, y_train)\n",
    "   print (\"Accuracy:\", my_ada.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. Implement the `fit` method. This is steps 1 and 2 from the AdaBoost.M1 algorithm.\n",
    "\n",
    "    You should have a for loop that calls your `_boost` method `n_estimators` times. Make sure to save all the estimators in `self.estimators_`. You also need to save all the estimator weights in `self.estimator_weight_`.\n",
    "\n",
    "4. Implement the `predict` method. This is step 3 from the algorithm.\n",
    "\n",
    "    Note that the algorithm considers the predictions to be either -1 or 1. So once you get predictions back from your Decision Trees, change the 0's to -1's.\n",
    "\n",
    "5. Implement the `score` method.\n",
    "\n",
    "    This should call the predict method and then calculate the accuracy.\n",
    "\n",
    "6. Load the file `data/spam_data.csv` into a dataframe. Use `train_test_split` to create test and train sets.\n",
    "   Train your implementation of `AdaBoostBinaryClassifier` on the train set and get the train and test accuracy scores.\n",
    "   Compare your results with sklearn's [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).\n",
    "   You should get approximately the same accuracy.\n",
    "\n",
    "   **Review the steps to implement the algorithm and make sure you (and your partner) have understood the underpinnings of boosting.**  \n",
    "\n",
    "<br>\n",
    "\n",
    "## Part 2: Estimator Complexity\n",
    "\n",
    "One of the assumptions of boosting is having weak classifiers at each of the boosting round. More complex classifiers would can lead to \"harder\" fits that make performance on a test set worse. Use sklearn's implementations from here on since that is what you would do in practice.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Refer to the function (`stage_score_plot`) you have implemented [this morning](individual.md). Tweak the function\n",
    "   to calculate and plot only the test misclassification from the estimator in each boosting round. Remember to use\n",
    "   the `staged_predicted` function that comes with the `AdaBoostClassifier` and `GradientBoostingClassifier`.\n",
    "\n",
    "2. Use `stage_score_plot` to plot the test error curve for `AdaBoostClassifier` and `GradientBoostingClassifier`.\n",
    "   In addition, include two more `GradientBoostingClassifier` models where the `max_depth` argument is `10` and `100`\n",
    "   respectively.\n",
    "\n",
    "   ![image](images/2_2_model_complexity.png)\n",
    "\n",
    "3. Compare the curves and comment on what is happening at `max_depth=100`\n",
    "\n",
    "4. GridSearch and tune your `GradientBoostingClassifier`.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Part 3: Feature importance and Partial Dependency Plots\n",
    "\n",
    "`GradientBoostingClassifier` has support for feature importance which informs how much each of the features explain\n",
    "the variance in the data. Partial Dependency Plots give you more detailed information about the relationship\n",
    "between each of the features and the response.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Make a feature importance plot by calling `feature_importances_` of your `GradientBoostingClassifier`.\n",
    "   Plot only the top 10 features which explain the most variance in the data. You will need to use `numpy.argsort()`\n",
    "   to order the feature importances and get the name of the top 10 most important features. Normalize the feature\n",
    "   importances by the maximum feature importance. Use `barh` in `matplotlib` to make the plot. Below is the snippet\n",
    "   for the plotting functionality.\n",
    "\n",
    "   ```python\n",
    "   import matplotlib.pyplot as plt\n",
    "   import numpy as np\n",
    "\n",
    "   fig = plt.figure(figsize=(10, 10))\n",
    "   x_ind = np.arange(feat_import.shape[0])\n",
    "   plt.barh(x_ind, feat_import, height=.3, align='center')\n",
    "   plt.ylim(x_ind.min() + .5, x_ind.max() + .5)\n",
    "   plt.yticks(x_ind, colnames, fontsize=14)\n",
    "   ```\n",
    "\n",
    "   ![image](images/3_1_feature_importance_bar_plot.png)\n",
    "\n",
    "2. Use [`plot_partial_dependence`](http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html)\n",
    "   to make partial dependence plot for the top 10 most important features. The `feature` argument should be the\n",
    "   indexes of the top 10 features from the `numpy.argsort()` you have done in `1.`. The `feature_names` argument should\n",
    "   be all the column names of the dataframe containing the spam data. Highlight and explain interesting trends you\n",
    "   observe in the PDP plots.\n",
    "\n",
    "   ![image](images/3_2_partial_dependency_plots.png)\n",
    "\n",
    "3. You can also examine the interaction between 2 variables by providing a list of tuples of 2 features for the `feature` argument\n",
    "   in `plot_partial_dependence`.\n",
    "\n",
    "   ![image](images/3_3_partial_dependency_plots.png)\n",
    "\n",
    "4. Furthermore, you can show the interaction of 2 variables and their joint relationship with the response variable by plotting\n",
    "   a 3-D surface. Adapt the code for this [example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html).\n",
    "   Do not worry about completely dissecting the code. It is a very important skill to be able to adapt other people's code\n",
    "   quickly to perform a task. Make a few plots where you may suspect there is an interaction and interpret the plots.\n",
    "\n",
    "   ![image](images/3_4_3D_partial_dependency_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
