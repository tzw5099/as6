{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Solution.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are a recursive divide and conquer algorithm. They are a non-linear, non-parametric discriminative supervised classification algorithm.  There are a few names of decision tree algorithms you may have heard of (ID3, C4.5, CART, etc.) and each is a different specification of a decision tree model.  You can read about them [here](http://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance) and [here](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).\n",
    "\n",
    "### Play Golf Dataset\n",
    "\n",
    "When implementing any ML algorithm for the first time, it is often easier to start with a trivially simple data set. You should always focus on one portion of the pipeline at a time: we do not want worry about cleaning data during feature selection just as we do not want to worry about feature engineering when writing our model building code.  We will be using the canonical 'Play Golf' [dataset](http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/dtrees/c4.5/c4.5_prob1.html) when writing our algorithm.\n",
    "\n",
    "Look at the [golf data](data/playgolf.csv). You will also see a dataset with just the categorial features and one with just the continuous features. Starting with just categorical features may be easier for implementation.\n",
    "\n",
    "### Pseudo-code\n",
    "\n",
    "Here's the pseudocode for the algorithm you will be implementing.\n",
    "\n",
    "    function BuildTree:\n",
    "        If every item in the dataset is in the same class\n",
    "        or there is no feature left to split the data:\n",
    "            return a leaf node with the class label\n",
    "        Else:\n",
    "            find the best feature and value to split the data\n",
    "            split the dataset\n",
    "            create a node\n",
    "            for each split\n",
    "                call BuildTree and add the result as a child of the node\n",
    "            return node\n",
    "\n",
    "### Implementation\n",
    "\n",
    "You've been given starter code in the [src](src) folder. Some of the instance variables chosen are not the only possible way of implementing a decision tree, so feel free to modify anything if it fits your implementation better.\n",
    "\n",
    "* The `TreeNode` class is implemented. These are the instance variables:\n",
    "\n",
    "    * `column` (int): index of feature to split on\n",
    "    * `split_value` (object): value of the feature to split on\n",
    "    * `categorical` (bool): whether or not node is split on a categorial feature (vs continuous)\n",
    "    * `name` (string): name of the feature (or name of the class in the case of a list)\n",
    "    * `left` (TreeNode): left child\n",
    "    * `right` (Tree Node): right child\n",
    "    * `leaf` (boolean): true or false depending on if the node is a leaf node.\n",
    "    * `classes` (Counter): if a leaf, a count of all the list of all the classes of the data points that terminate at this leaf.  Can be used to assess how \"accurate\" an individual leaf is.\n",
    "\n",
    "    The `as_string` and `__str__` functions are designed to print out the decision tree (mostly for debugging).\n",
    "\n",
    "* There is starter code for the `DecisionTree` class. You will need to fill in the class so that you can use your decision tree code as follows (also see `src/run_decision_tree.py`).\n",
    "\n",
    "    ```python\n",
    "    tree = DecisionTree()\n",
    "    tree.fit(X, y, df.columns[:-1])\n",
    "    print tree\n",
    "    y_predict = tree.predict(X)\n",
    "    ```\n",
    "\n",
    "    You can see that the `__str__` method is implemented for you. This enables you to print your tree for debugging purposes.\n",
    "\n",
    "* The `__init__`, `fit`, `_build_tree` and `__str__` methods are already implemented for you. You will need to implement the other ones.\n",
    "\n",
    "* There are minimal tests in `src/test_decision_tree.py`. One test for each method you need to implement. You can run the tests with this command:\n",
    "\n",
    "    ```\n",
    "    nosetests src/test_decision_tree.py\n",
    "    ```\n",
    "\n",
    "* The file `run_decision_tree.py` should run your Decision Tree code, print the resulting decision tree and show the predicted results.\n",
    "\n",
    "### Steps to Implementing\n",
    "\n",
    "We will be implementing the **CART** algorithm. This means that every split will be binary. For categorical features, splits will be like: `sunny` or `not sunny`. For continuous features, splits will be like: `<80` or `>=80`.\n",
    "\n",
    "1. Implement the `_entropy` method, which is given by the following equation. Entropy measures the amount of \"disorder\" in a set. Here there are *m* classes in the set and *ci* is the *i*-th class of our target y.\n",
    "\n",
    "    ![shannon entropy](images/entropy.png)\n",
    "\n",
    "    *P(c)* = (count of occurrences of class *c*) / size of *y*\n",
    "\n",
    "    Note that to calculate entropy, you only need the labels (`y` values) and none of the feature values.\n",
    "\n",
    "2. Implement the `_gini` method. Your information gain method will be able to use either gini or entropy.\n",
    "\n",
    "    ![gini impurity](images/gini.png)\n",
    "\n",
    "3. Implement the `_make_split` method. This should take the index of the feature and the value of the feature and make the split of the data into two subsets. Note that for categorical features this should split on whether it's equal to the value or not. For continuous, it should split on `<` or `>=`.\n",
    "\n",
    "4. Implement the `_information_gain` method. This should take a split (the result of the `_make_split` method) and return the value of the information gain.\n",
    "\n",
    "5. Implement the `_choose_split_index` method. This should take the data and try every possible feature and value to split on. It should find the one with the best information gain.\n",
    "\n",
    "6. The `predict` method in the Decision Tree class is implemented by calling the `predict_one` method in the `TreeNode` class. You need to finish the implementation of the `predict_one` method by giving the conditions for when you should move left or right on the decision tree.\n",
    "\n",
    "\n",
    "### Extra Credit 1: Pruning\n",
    "\n",
    "*Pruning* is designed to simplify the tree so it doesn't go so deep. It is a way of stopping earlier or merging leaves that helps deal with overfitting. The first two extra credit problems are implementing prepruning and postpruning. A well designed decision tree would have these implemented.\n",
    "\n",
    "1. *Prepruning* is making the decision tree algorithm stop early. Here are a few ways that we preprune:\n",
    "    * leaf size: Stop when the number of data points for a leaf gets below a threshold\n",
    "    * depth: Stop when the depth of the tree (distance from root to leaf) reaches a threshold\n",
    "    * mostly the same: Stop when some percent of the data points are the same (rather than all the same)\n",
    "    * error threshold: Stop when the error reduction (information gain) isn't improved significantly.\n",
    "\n",
    "    Implement some of the prepruning thresholds and play around with using them.\n",
    "\n",
    "2. Implement *postpruning* for your decision tree. You build the tree the same as before, but after you've built the tree, merge some nodes together if doing so reduces the test-set error. Here's the psuedocode:\n",
    "\n",
    "        function Prune:\n",
    "            if either left or right is not a leaf:\n",
    "                call Prune on that split\n",
    "            if both left and right are leaf nodes:\n",
    "                calculate error associated with merging two nodes\n",
    "                calculate error associated without merging two nodes\n",
    "                if merging results in lower error:\n",
    "                    merge the leaf nodes\n",
    "\n",
    "    You can find more detail in section 9.4.2 in Machine Learning in Action.\n",
    "\n",
    "\n",
    "### Extra Credit 2: Decision Trees for Regression\n",
    "\n",
    "**Note:** Before starting this, make sure you commit your code with a `git commit`! Don't lose your past results with your new changes!\n",
    "\n",
    "You can use decision trees for predicting continuous values as well. Instead of using entropy to calculate the disorder in the set, we use the variance.\n",
    "\n",
    "To get to value of a leaf node, average all of the values.\n",
    "\n",
    "1. Make your decision tree able to predict continuous values. You can modify your decision tree class so that it can do either continuous or categorical depending on what parameters you pass it, or just copy and create a new class. For checking out if your code is implemented correctly, you can use the same dataset and predict one of the continuous variables.\n",
    "\n",
    "2. Implement model trees, which are predictors which start by using a decision tree, but use linear regression to predict the value on each leaf node. Details can be found in 9.5 of Machine Learning in Action.\n",
    "\n",
    "\n",
    "### Extra Credit 3: A Real Dataset\n",
    "\n",
    "1. Try running your decision tree code on a previous exercise's dataset.\n",
    "\n",
    "2. Use sklearn's [Decision Tree](http://scikit-learn.org/stable/modules/tree.html#classification) and [k Nearest Neighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classifiers on the same dataset. How well do they do compared to logistic regression?\n",
    "\n",
    "3. Implement model trees, which are predictors which start by using a decision tree, but use linear regression to predict the value on each leaf node. Details can be found in 9.5 of Machine Learning in Action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:11:42.584939Z",
     "start_time": "2018-04-11T21:11:41.056074Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomwong/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# COLLAPSE CELL\n",
    "from collections import Counter\n",
    "from TreeNode import TreeNode\n",
    "from DecisionTreePruning import DecisionTreePruning\n",
    "from itertools import izip\n",
    "from DecisionTreeRegressor import DecisionTreeRegressor\n",
    "# PMsearch np.v*\n",
    "#x = data['mass']\n",
    "#x?\n",
    "\n",
    "# from jupyterthemes import jtplot\n",
    "# jtplot.style(theme='solarized')\n",
    "# from jupyterlab_table import JSONTable\n",
    "# JSONTable(df)\n",
    "\n",
    "# from IPython.display import HTML, display\n",
    "\n",
    "# from notebook.services.config import ConfigManager\n",
    "# cm = ConfigManager().update('notebook', {'limit_output': 1000})\n",
    "\n",
    "import better_exceptions\n",
    "better_exceptions.MAX_LENGTH = None\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from pprint import pprint\n",
    "import math\n",
    "import statsmodels.stats as sms\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.regression as smr\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# 04atplotlib inline\n",
    "# %load_ext heat\n",
    "\n",
    "plt.ion()\n",
    "# plt.ioff()\n",
    "\n",
    "# %heat\n",
    "\n",
    "import os \n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:30:32.042375Z",
     "start_time": "2018-04-11T21:30:31.954515Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TreeNode.py\n",
    "\"\"\"Contains a node class for a decision tree.\"\"\"\n",
    "\n",
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "class TreeNode(object):\n",
    "    \"\"\"A node class for a decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize an empty TreeNode.\"\"\"\n",
    "        self.column = None  # (int)    index of feature to split on\n",
    "        self.value = None  # value of the feature to split on\n",
    "        self.categorical = True  # (bool) whether or not node is split on\n",
    "                                 # categorial feature\n",
    "        self.name = None    # (string) name of feature (or name of class in the\n",
    "                            #          case of a list)\n",
    "        self.left = None    # (TreeNode) left child\n",
    "        self.right = None   # (TreeNode) right child\n",
    "        self.leaf = False   # (bool)   true if node is a leaf, false otherwise\n",
    "        self.classes = Counter()  # (Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        \"\"\"Return the predicted label for a single data point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: 1d numpy array\n",
    "            A single data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y: predicted label\n",
    "        \"\"\"\n",
    "        if self.leaf:\n",
    "            return self.name\n",
    "        col_value = x[self.column]\n",
    "\n",
    "        if self.categorical:\n",
    "            if col_value == self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "        else:\n",
    "            if col_value < self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "\n",
    "    # This is for visualizing your tree. You don't need to look into this code.\n",
    "    def as_string(self, level=0, prefix=\"\"):\n",
    "        \"\"\"Return a string representation of the tree rooted at this node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        level: int\n",
    "            Amount to indent.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prefix: str\n",
    "            Prefix to start the line with.\n",
    "        \"\"\"\n",
    "        result = \"\"\n",
    "        if prefix:\n",
    "            indent = \"  |   \" * (level - 1) + \"  |-> \"\n",
    "            result += indent + prefix + \"\\n\"\n",
    "        indent = \"  |   \" * level\n",
    "        result += indent + \"  \" + str(self.name) + \"\\n\"\n",
    "        if not self.leaf:\n",
    "            if self.categorical:\n",
    "                left_key = str(self.value)\n",
    "                right_key = \"no \" + str(self.value)\n",
    "            else:\n",
    "                left_key = \"< \" + str(self.value)\n",
    "                right_key = \">= \" + str(self.value)\n",
    "            result += self.left.as_string(level + 1, left_key + \":\")\n",
    "            result += self.right.as_string(level + 1, right_key + \":\")\n",
    "        return result\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Represent TreeNode as string.\"\"\"\n",
    "        return self.as_string().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:30:33.870702Z",
     "start_time": "2018-04-11T21:30:33.507653Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DecisionTree.py\n",
    "\n",
    "\"\"\"\n",
    "Class implementing the CART decision tree algorithm.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# from collections import Counter\n",
    "# from TreeNode import TreeNode\n",
    "\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Classifier implementing the CART decision tree algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    impurity_criterion: string, optional (default='entropy')\n",
    "        String indicating the impurity_criterion to use.\n",
    "        Use 'gini' to have tree us Gini impurity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy'):\n",
    "        \"\"\"Initialize an empty DecisionTree.\"\"\"\n",
    "        # Root Node\n",
    "        self.root = None\n",
    "        # String names of features (for interpreting the tree)\n",
    "        self.feature_names = None\n",
    "        # Boolean array of whether variable is categorical (or continuous)\n",
    "        self.categorical = None\n",
    "\n",
    "        if impurity_criterion == 'entropy':\n",
    "            self.impurity_criterion = self._entropy\n",
    "        else:\n",
    "            self.impurity_criterion = self._gini\n",
    "\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"Build the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "            The training labels.\n",
    "        feature_names: numpy array, optional (default=None)\n",
    "            Array of strings containing names of each of the features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # Create True/False array of whether the variable is categorical\n",
    "        is_categorical = lambda x: isinstance(x, str) or isinstance(x, bool)\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        \"\"\"Recursively build the decision tree.\n",
    "\n",
    "        Return the root node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TreeNode\n",
    "        \"\"\"\n",
    "        node = TreeNode()\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            node.leaf = True\n",
    "            node.classes = Counter(y)\n",
    "            node.name = node.classes.most_common(1)[0][0]\n",
    "        else:\n",
    "            X1, y1, X2, y2 = splits\n",
    "            node.column = index\n",
    "            node.name = self.feature_names[index]\n",
    "            node.value = value\n",
    "            node.categorical = self.categorical[index]\n",
    "            node.left = self._build_tree(X1, y1)\n",
    "            node.right = self._build_tree(X2, y2)\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Return the entropy of the array y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            An array of data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Entropy of the array y.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        summation = 0\n",
    "        for c_i in np.unique(y):\n",
    "            prob = np.mean(y == c_i)\n",
    "            summation += prob * np.log2(prob)\n",
    "        return -summation\n",
    "\n",
    "    def _gini(self, y):\n",
    "        \"\"\"Return the gini impurity of the array y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            An array of data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Gini impurity of the array y.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        summation = 0\n",
    "        for c_i in np.unique(y):\n",
    "            prob = np.mean(y == c_i)\n",
    "            summation += prob**2\n",
    "        return 1 - summation\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        \"\"\"Return the subsets of the dataset for the given split index & value.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            Feature matrix.\n",
    "        y: 1d numpy array\n",
    "            Label matrix.\n",
    "        split_index: int\n",
    "            Index of the feature to split on.\n",
    "        split_value: int/float/bool/str\n",
    "            Value of feature to split on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X1: 2d numpy array\n",
    "            Feature matrix for subset 1\n",
    "        y1: 1d numpy array\n",
    "            Labels for subset 1\n",
    "        X2: 2d numpy array\n",
    "            Feature matrix for subset 2\n",
    "        y2: 1d numpy array\n",
    "            Labels for subset 2\n",
    "        \"\"\"\n",
    "        if self.categorical[split_index]:\n",
    "            idx = X[:, split_index] == split_value\n",
    "        else:\n",
    "            idx = X[:, split_index] < split_value\n",
    "        return X[idx], y[idx], X[~idx], y[~idx]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        \"\"\"Return the information gain of making the given split.\n",
    "\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            Labels for parent node.\n",
    "        y1: 1d numpy array\n",
    "            Labels for subset node 1.\n",
    "        y2: 1d numpy array\n",
    "            Labels for subset node 2.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The information gain of making the given split.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        weighted_child_imp = 0\n",
    "        for y_i in (y1, y2):\n",
    "            weighted_child_imp += self.impurity_criterion(y_i) * y_i.shape[0] / n\n",
    "        return self.impurity_criterion(y) - weighted_child_imp\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        \"\"\"Return the index and value of the feature to split on.\n",
    "\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> index, value, splits = self._choose_split_index(X, y)\n",
    "        >>> X1, y1, X2, y2 = splits\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        index: int\n",
    "            Index of feature\n",
    "        value: int/float/bool/str\n",
    "            Value of feature\n",
    "        splits: tuple\n",
    "            (2d array, 1d array, 2d array, 1d array)\n",
    "        \"\"\"\n",
    "        split_index, split_value, splits = None, None, None\n",
    "        best_gain = 0\n",
    "        for i in range(X.shape[1]):\n",
    "            values = np.unique(X[:, i])\n",
    "            if len(values) <= 1:\n",
    "                continue\n",
    "            for value in values:\n",
    "                X1, y1, X2, y2 = self._make_split(X, y, i, value)\n",
    "                gain = self._information_gain(y, y1, y2)\n",
    "                if gain > best_gain:\n",
    "                    split_index = i\n",
    "                    split_value = value\n",
    "                    splits = (X1, y1, X2, y2)\n",
    "                    best_gain = gain\n",
    "        return split_index, split_value, splits\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return an array of predictions for the feature matrix X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            The feature matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y: 1d numpy array\n",
    "            Matrix of predicted labels.\n",
    "        \"\"\"\n",
    "        return np.array([self.root.predict_one(row) for row in X])\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return string representation of the Decision Tree.\"\"\"\n",
    "        return str(self.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:30:50.966116Z",
     "start_time": "2018-04-11T21:30:50.032853Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DecisionTreePruning.py\n",
    "\n",
    "\"\"\"\n",
    "CART decision tree algorithm with pre/post pruning implemented.\n",
    "\n",
    "TODO: Clean up multi-line lambda function (not pep8 standards)\n",
    "\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# from collections import Counter\n",
    "# from TreeNode import TreeNode\n",
    "\n",
    "\n",
    "class DecisionTreePruning(object):\n",
    "    \"\"\"Classifier decision tree with pre and post pruning implemented.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    impurity_criterion: string, optional (default='entropy')\n",
    "        String indicating the impurity_criterion to use.\n",
    "        Use 'gini' to have tree us Gini impurity.\n",
    "\n",
    "    leaf_size: int, optional (default=None)\n",
    "        The maxiumum number of samples in a leaf. Pre-pruning parameter.\n",
    "\n",
    "    depth: int, optional (default=None)\n",
    "        The maximum depth the tree should reach. Pre-pruning parameter.\n",
    "\n",
    "    same_ratio: float, optional (default=None)\n",
    "        The maximum ratio for instances of the same class in a leaf node.\n",
    "        Pre-pruning parameter.\n",
    "\n",
    "    error_threshold: float, optional (default=None)\n",
    "        The minimum information gain threshold for a split.\n",
    "        Pre-pruning parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy', leaf_size=None,\n",
    "                 depth=None, same_ratio=None, error_threshold=None):\n",
    "        \"\"\"Initialize an empty DecisionTreePruning.\"\"\"\n",
    "        # Root Node\n",
    "        self.root = None\n",
    "        # String names of features (for interpreting the tree)\n",
    "        self.feature_names = None\n",
    "        # Boolean array of whether variable is categorical (or continuous)\n",
    "        self.categorical = None\n",
    "\n",
    "        if impurity_criterion == 'entropy':\n",
    "            self.impurity_criterion = self._entropy\n",
    "        else:\n",
    "            self.impurity_criterion = self._gini\n",
    "\n",
    "        self.leaf_size = leaf_size\n",
    "        self.depth = depth\n",
    "        self.same_ratio = same_ratio\n",
    "        self.error_threshold = error_threshold\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"Build the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "            The training labels.\n",
    "        feature_names: numpy array, optional (default=None)\n",
    "            Array of strings containing names of each of the features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # Create True/False array of whether the variable is categorical\n",
    "        is_categorical = lambda x: isinstance(x, str) or \\\n",
    "                                   isinstance(x, bool) or \\\n",
    "                                   isinstance(x, unicode)\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _pre_prune(self, y, splits, depth):\n",
    "        \"\"\"Return True if any stopping threshold has been reached.\n",
    "\n",
    "        Check pre-prunning parameters and return True/False if any stopping\n",
    "        threshold has been reached.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            Parent node\n",
    "        splits: tuple of numpy arrays\n",
    "            Child nodes, tuple = (X1, y1, X2, y2)\n",
    "        depth: int\n",
    "            Maximum depth to build tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Boolean\n",
    "        \"\"\"\n",
    "        X1, y1, X2, y2 = splits\n",
    "\n",
    "        if self.leaf_size is not None:\n",
    "            if self.leaf_size >= X1.shape[0] or self.leaf_size >= X2.shape[0]:\n",
    "                return True\n",
    "        elif self.depth is not None and depth >= self.depth:\n",
    "            return True\n",
    "        elif self.same_ratio is not None:\n",
    "            y1_ratio = Counter(y1).most_common(1) / float(y1.shape[0])\n",
    "            y2_ratio = Counter(y2).most_common(1) / float(y2.shape[0])\n",
    "            if y1_ratio >= self.same_ratio or y2_ratio >= self.same_ratio:\n",
    "                return True\n",
    "        elif self.error_threshold is not None:\n",
    "            if self.error_threshold > self._information_gain(y, y1, y2):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _build_tree(self, X, y, depth=-1):\n",
    "        \"\"\"Recursively build the decision tree.\n",
    "\n",
    "        Return the root node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TreeNode\n",
    "        \"\"\"\n",
    "        depth += 1\n",
    "\n",
    "        node = TreeNode()\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        if splits is not None:\n",
    "            preprune = self._pre_prune(y, splits, depth)\n",
    "        else:\n",
    "            preprune = False\n",
    "\n",
    "        if index is None or len(np.unique(y)) == 1 or preprune:\n",
    "            node.leaf = True\n",
    "            node.classes = Counter(y)\n",
    "            node.name = node.classes.most_common(1)[0][0]\n",
    "        else:\n",
    "            X1, y1, X2, y2 = splits\n",
    "            node.column = index\n",
    "            node.name = self.feature_names[index]\n",
    "            node.value = value\n",
    "            node.categorical = self.categorical[index]\n",
    "            node.left = self._build_tree(X1, y1, depth)\n",
    "            node.right = self._build_tree(X2, y2, depth)\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Return the entropy of the array y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            An array of data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Entropy of the array y.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        summation = 0\n",
    "        for c_i in np.unique(y):\n",
    "            prob = sum(y == c_i) / float(n)\n",
    "            summation += prob * np.log2(prob)\n",
    "        return -summation\n",
    "\n",
    "    def _gini(self, y):\n",
    "        \"\"\"Return the gini impurity of the array y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            An array of data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Gini impurity of the array y.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        summation = 0\n",
    "        for c_i in np.unique(y):\n",
    "            prob = sum(y == c_i) / float(n)\n",
    "            summation += prob**2\n",
    "        return 1 - summation\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        \"\"\"Return the subsets of the dataset for the given split index & value.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            Feature matrix.\n",
    "        y: 1d numpy array\n",
    "            Label matrix.\n",
    "        split_index: int\n",
    "            Index of the feature to split on.\n",
    "        split_value: int/float/bool/str\n",
    "            Value of feature to split on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X1: 2d numpy array\n",
    "            Feature matrix for subset 1\n",
    "        y1: 1d numpy array\n",
    "            Labels for subset 1\n",
    "        X2: 2d numpy array\n",
    "            Feature matrix for subset 2\n",
    "        y2: 1d numpy array\n",
    "            Labels for subset 2\n",
    "        \"\"\"\n",
    "        if self.categorical[split_index]:\n",
    "            idx = X[:, split_index] == split_value\n",
    "        else:\n",
    "            idx = X[:, split_index] < split_value\n",
    "        return X[idx], y[idx], X[idx == False], y[idx == False]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        \"\"\"Return the information gain of making the given split.\n",
    "\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            Labels for parent node.\n",
    "        y1: 1d numpy array\n",
    "            Labels for subset node 1.\n",
    "        y2: 1d numpy array\n",
    "            Labels for subset node 2.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The information gain of making the given split.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        child_inf = 0\n",
    "        for y_i in (y1, y2):\n",
    "            child_inf += self.impurity_criterion(y_i) * y_i.shape[0] / float(n)\n",
    "        return self.impurity_criterion(y) - child_inf\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        \"\"\"Return the index and value of the feature to split on.\n",
    "\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> index, value, splits = self._choose_split_index(X, y)\n",
    "        >>> X1, y1, X2, y2 = splits\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        index: int\n",
    "            Index of feature\n",
    "        value: int/float/bool/str\n",
    "            Value of feature\n",
    "        splits: tuple\n",
    "            (2d array, 1d array, 2d array, 1d array)\n",
    "        \"\"\"\n",
    "        split_index, split_value, splits = None, None, None\n",
    "        gain = 0\n",
    "        for i in xrange(X.shape[1]):\n",
    "            values = np.unique(X[:, i])\n",
    "            if len(values) < 1:\n",
    "                continue\n",
    "            for value in values:\n",
    "                X1, y1, X2, y2 = self._make_split(X, y, i, value)\n",
    "                new_gain = self._information_gain(y, y1, y2)\n",
    "                if new_gain > gain:\n",
    "                    split_index = i\n",
    "                    split_value = value\n",
    "                    splits = (X1, y1, X2, y2)\n",
    "                    gain = new_gain\n",
    "        return split_index, split_value, splits\n",
    "\n",
    "    def prune(self, X, y, node=None):\n",
    "        \"\"\"Post-prune tree by merging leaves using error rate.\n",
    "\n",
    "        Recursively checks for leaves and compares error rate before and after\n",
    "        merging the leaves.  If merged improves error rate, merge leaves.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, 2]\n",
    "            Feature matrix.\n",
    "        y: 1d numpy array, [n_samples]\n",
    "            Label matrix.\n",
    "        node: TreeNode root, optional (default=None)\n",
    "            The root TreeNode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        if not node.left.leaf:\n",
    "            self.prune(X, y, node.left)\n",
    "\n",
    "        if not node.right.leaf:\n",
    "            self.prune(X, y, node.right)\n",
    "\n",
    "        if node.left.leaf and node.right.leaf:\n",
    "            leaf_y = self._predict(X, node)\n",
    "            merged_classes = node.left.classes + node.right.classes\n",
    "            merged_name = merged_classes.most_common(1)[0][0]\n",
    "            merged_y = np.array([merged_name] * y.shape[0])\n",
    "            leaf_score = sum(leaf_y == y) / float(y.shape[0])\n",
    "            merged_score = sum(merged_y == y) / float(y.shape[0])\n",
    "\n",
    "            if merged_score >= leaf_score:\n",
    "                print ('Merging')\n",
    "                node.leaf = True\n",
    "                node.classes = merged_classes\n",
    "                node.name = merged_name\n",
    "                node.left = None\n",
    "                node.right = None\n",
    "\n",
    "    def _predict(self, X, node):\n",
    "        \"\"\"Return prediction to calculate error rate for post-pruning.\"\"\"\n",
    "        return np.array([node.predict_one(row) for row in X])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return an array of predictions for the feature matrix X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            The feature matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y: 1d numpy array\n",
    "            Matrix of predicted labels.\n",
    "        \"\"\"\n",
    "        return np.array([self.root.predict_one(row) for row in X])\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return string representation of the Decision Tree.\"\"\"\n",
    "        return str(self.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:30:55.490420Z",
     "start_time": "2018-04-11T21:30:54.807670Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor.py\n",
    "\"\"\"\n",
    "Regression tree algorithm with pre/post pruning implemented.\n",
    "\n",
    "TODO: Clean up multi-line lambda function (not pep8 standards)\n",
    "\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# from collections import Counter\n",
    "# from TreeNode import TreeNode\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor(object):\n",
    "    \"\"\"Regression tree class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    leaf_size: int, optional (default=None)\n",
    "        The maxiumum number of samples in a leaf. Pre-pruning parameter.\n",
    "\n",
    "    depth: int, optional (default=None)\n",
    "        The maximum depth the tree should reach. Pre-pruning parameter.\n",
    "\n",
    "    same_ratio: float, optional (default=None)\n",
    "        The maximum ratio for instances of the same class in a leaf node.\n",
    "        Pre-pruning parameter.\n",
    "\n",
    "    error_threshold: float, optional (default=None)\n",
    "        The minimum information gain threshold for a split.\n",
    "        Pre-pruning parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, leaf_size=None, depth=None, same_ratio=None,\n",
    "                 error_threshold=None):\n",
    "        \"\"\"Initialize an empty DecisionTreeRegressor.\"\"\"\n",
    "        # Root Node\n",
    "        self.root = None\n",
    "        # String names of features (for interpreting the tree)\n",
    "        self.feature_names = None\n",
    "        # Boolean array of whether variable is categorical (or continuous)\n",
    "        self.categorical = None\n",
    "\n",
    "        self.leaf_size = leaf_size\n",
    "        self.depth = depth\n",
    "        self.same_ratio = same_ratio\n",
    "        self.error_threshold = error_threshold\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"Build the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "            The training labels.\n",
    "        feature_names: numpy array, optional (default=None)\n",
    "            Array of strings containing names of each of the features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # Create True/False array of whether the variable is categorical\n",
    "        is_categorical = lambda x: isinstance(x, str) or \\\n",
    "                                   isinstance(x, bool) or \\\n",
    "                                   isinstance(x, unicode)\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _pre_prune(self, y, splits, depth):\n",
    "        \"\"\"Return True if any stopping threshold has been reached.\n",
    "\n",
    "        Check pre-prunning parameters and return True/False if any stopping\n",
    "        threshold has been reached.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            Parent node\n",
    "        splits: tuple of numpy arrays\n",
    "            Child nodes, tuple = (X1, y1, X2, y2)\n",
    "        depth: int\n",
    "            Maximum depth to build tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Boolean\n",
    "        \"\"\"\n",
    "        X1, y1, X2, y2 = splits\n",
    "\n",
    "        if self.leaf_size is not None:\n",
    "            if self.leaf_size >= X1.shape[0] or self.leaf_size >= X2.shape[0]:\n",
    "                return True\n",
    "        elif self.depth is not None and depth >= self.depth:\n",
    "            return True\n",
    "        elif self.same_ratio is not None:\n",
    "            y1_ratio = Counter(y1).most_common(1) / float(y1.shape[0])\n",
    "            y2_ratio = Counter(y2).most_common(1) / float(y2.shape[0])\n",
    "            if y1_ratio >= self.same_ratio or y2_ratio >= self.same_ratio:\n",
    "                return True\n",
    "        elif self.error_threshold is not None:\n",
    "            if self.error_threshold > self._information_gain(y, y1, y2):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _build_tree(self, X, y, depth=-1):\n",
    "        \"\"\"Recursively build the decision tree.\n",
    "\n",
    "        Return the root node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TreeNode\n",
    "        \"\"\"\n",
    "        depth += 1\n",
    "\n",
    "        node = TreeNode()\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        if splits is not None:\n",
    "            preprune = self._pre_prune(y, splits, depth)\n",
    "        else:\n",
    "            preprune = False\n",
    "\n",
    "        if index is None or len(np.unique(y)) == 1 or preprune:\n",
    "            node.leaf = True\n",
    "            node.classes = y\n",
    "            node.name = y.mean()\n",
    "        else:\n",
    "            X1, y1, X2, y2 = splits\n",
    "            node.column = index\n",
    "            node.name = self.feature_names[index]\n",
    "            node.value = value\n",
    "            node.categorical = self.categorical[index]\n",
    "            node.left = self._build_tree(X1, y1, depth)\n",
    "            node.right = self._build_tree(X2, y2, depth)\n",
    "        return node\n",
    "\n",
    "    def _standard_deviaton(self, y):\n",
    "        \"\"\"Return the _standard_deviaton of the array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            Array of targets to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "        \"\"\"\n",
    "        y_mean = y.mean()\n",
    "        n = y.shape[0]\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return np.sqrt(sum((y - y_mean)**2) / float(n))\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        \"\"\"Return the subsets of the dataset for the given split index & value.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            Feature matrix.\n",
    "        y: 1d numpy array\n",
    "            Label matrix.\n",
    "        split_index: int\n",
    "            Index of the feature to split on.\n",
    "        split_value: int/float/bool/str\n",
    "            Value of feature to split on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X1: 2d numpy array\n",
    "            Feature matrix for subset 1\n",
    "        y1: 1d numpy array\n",
    "            Labels for subset 1\n",
    "        X2: 2d numpy array\n",
    "            Feature matrix for subset 2\n",
    "        y2: 1d numpy array\n",
    "            Labels for subset 2\n",
    "        \"\"\"\n",
    "        if self.categorical[split_index]:\n",
    "            idx = X[:, split_index] == split_value\n",
    "        else:\n",
    "            idx = X[:, split_index] < split_value\n",
    "        return X[idx], y[idx], X[idx == False], y[idx == False]\n",
    "\n",
    "    def _std_reduction(self, y, y1, y2):\n",
    "        \"\"\"Return the standard deviation reduction of making the given split.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            Targets for parent node.\n",
    "        y1: 1d numpy array\n",
    "            Targets for subset 1.\n",
    "        y2: 1d numpy array\n",
    "            Targets for subset 2.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        child_std = 0\n",
    "        for y_i in (y1, y2):\n",
    "            child_std += self._standard_deviaton(y_i) * y_i.shape[0] / float(n)\n",
    "        return self._standard_deviaton(y) - child_std\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        \"\"\"Return the index and value of the feature to split on.\n",
    "\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> index, value, splits = self._choose_split_index(X, y)\n",
    "        >>> X1, y1, X2, y2 = splits\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        index: int\n",
    "            Index of feature\n",
    "        value: int/float/bool/str\n",
    "            Value of feature\n",
    "        splits: tuple\n",
    "            (2d array, 1d array, 2d array, 1d array)\n",
    "        \"\"\"\n",
    "        split_index, split_value, splits = None, None, None\n",
    "        std_reduction = 0\n",
    "        for i in xrange(X.shape[1]):\n",
    "            values = np.unique(X[:, i])\n",
    "            if len(values) < 1:\n",
    "                continue\n",
    "            for value in values:\n",
    "                X1, y1, X2, y2 = self._make_split(X, y, i, value)\n",
    "                new_std_reduction = self._std_reduction(y, y1, y2)\n",
    "                if new_std_reduction > std_reduction:\n",
    "                    split_index = i\n",
    "                    split_value = value\n",
    "                    splits = (X1, y1, X2, y2)\n",
    "                    std_reduction = new_std_reduction\n",
    "        return split_index, split_value, splits\n",
    "\n",
    "    def prune(self, X, y, node=None):\n",
    "        \"\"\"Post-prune tree by merging leaves using error rate.\n",
    "\n",
    "        Recursively checks for leaves and compares error rate before and after\n",
    "        merging the leaves.  If merged improves error rate, merge leaves.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, 2]\n",
    "            Feature matrix.\n",
    "        y: 1d numpy array, [n_samples]\n",
    "            Label matrix.\n",
    "        node: TreeNode root, optional (default=None)\n",
    "            The root TreeNode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        if not node.left.leaf:\n",
    "            self.prune(X, y, node.left)\n",
    "\n",
    "        if not node.right.leaf:\n",
    "            self.prune(X, y, node.right)\n",
    "\n",
    "        if node.left.leaf and node.right.leaf:\n",
    "            leaf_y = self.predict(X)\n",
    "            merged_classes = np.concatenate((node.left.classes,\n",
    "                                             node.right.classes))\n",
    "            merged_name = merged_classes.mean()\n",
    "            merged_y = merged_name\n",
    "            leaf_score = sum((y - leaf_y)**2)\n",
    "            merged_score = sum((y - merged_y)**2)\n",
    "\n",
    "            if merged_score <= leaf_score:\n",
    "                node.leaf = True\n",
    "                node.classes = merged_classes\n",
    "                node.name = merged_name\n",
    "                node.left = None\n",
    "                node.right = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return an array of predictions for the feature matrix X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            The feature matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y: 1d numpy array\n",
    "            Matrix of predicted labels.\n",
    "        \"\"\"\n",
    "        return np.apply_along_axis(self.root.predict_one, axis=1, arr=X)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return string representation of the Decision Tree.\"\"\"\n",
    "        return str(self.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:10:38.378022Z",
     "start_time": "2018-04-11T21:10:37.860975Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run_decision_tree.py\n",
    "\"\"\"\n",
    "Class implementing the CART decision tree algorithm.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import math\n",
    "# from collections import Counter\n",
    "# from TreeNode import TreeNode\n",
    "\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Classifier implementing the CART decision tree algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    impurity_criterion: string, optional (default='entropy')\n",
    "        String indicating the impurity_criterion to use.\n",
    "        Use 'gini' to have tree us Gini impurity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy'):\n",
    "        \"\"\"Initialize an empty DecisionTree.\"\"\"\n",
    "        # Root Node\n",
    "        self.root = None\n",
    "        # String names of features (for interpreting the tree)\n",
    "        self.feature_names = None\n",
    "        # Boolean array of whether variable is categorical (or continuous)\n",
    "        self.categorical = None\n",
    "\n",
    "        if impurity_criterion == 'entropy':\n",
    "            self.impurity_criterion = self._entropy\n",
    "        else:\n",
    "            self.impurity_criterion = self._gini\n",
    "\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"Build the decision tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "            The training labels.\n",
    "        feature_names: numpy array, optional (default=None)\n",
    "            Array of strings containing names of each of the features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # Create True/False array of whether the variable is categorical\n",
    "        is_categorical = lambda x: isinstance(x, str) or isinstance(x, bool)\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        \"\"\"Recursively build the decision tree.\n",
    "\n",
    "        Return the root node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array, shape = [n_samples, n_features]\n",
    "            The training data.\n",
    "        y: 1d numpy array, shape = [n_samples]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TreeNode\n",
    "        \"\"\"\n",
    "        node = TreeNode()\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            node.leaf = True\n",
    "            node.classes = Counter(y)\n",
    "            node.name = node.classes.most_common(1)[0][0]\n",
    "        else:\n",
    "            X1, y1, X2, y2 = splits\n",
    "            node.column = index\n",
    "            node.name = self.feature_names[index]\n",
    "            node.value = value\n",
    "            node.categorical = self.categorical[index]\n",
    "            node.left = self._build_tree(X1, y1)\n",
    "            node.right = self._build_tree(X2, y2)\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Return the entropy of the array y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            An array of data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Entropy of the array y.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        summation = 0\n",
    "        for c_i in np.unique(y):\n",
    "            prob = np.mean(y == c_i)\n",
    "            summation += prob * np.log2(prob)\n",
    "        return -summation\n",
    "\n",
    "    def _gini(self, y):\n",
    "        \"\"\"Return the gini impurity of the array y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            An array of data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Gini impurity of the array y.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        summation = 0\n",
    "        for c_i in np.unique(y):\n",
    "            prob = np.mean(y == c_i)\n",
    "            summation += prob**2\n",
    "        return 1 - summation\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        \"\"\"Return the subsets of the dataset for the given split index & value.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            Feature matrix.\n",
    "        y: 1d numpy array\n",
    "            Label matrix.\n",
    "        split_index: int\n",
    "            Index of the feature to split on.\n",
    "        split_value: int/float/bool/str\n",
    "            Value of feature to split on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X1: 2d numpy array\n",
    "            Feature matrix for subset 1\n",
    "        y1: 1d numpy array\n",
    "            Labels for subset 1\n",
    "        X2: 2d numpy array\n",
    "            Feature matrix for subset 2\n",
    "        y2: 1d numpy array\n",
    "            Labels for subset 2\n",
    "        \"\"\"\n",
    "        if self.categorical[split_index]:\n",
    "            idx = X[:, split_index] == split_value\n",
    "        else:\n",
    "            idx = X[:, split_index] < split_value\n",
    "        return X[idx], y[idx], X[~idx], y[~idx]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        \"\"\"Return the information gain of making the given split.\n",
    "\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: 1d numpy array\n",
    "            Labels for parent node.\n",
    "        y1: 1d numpy array\n",
    "            Labels for subset node 1.\n",
    "        y2: 1d numpy array\n",
    "            Labels for subset node 2.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The information gain of making the given split.\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        weighted_child_imp = 0\n",
    "        for y_i in (y1, y2):\n",
    "            weighted_child_imp += self.impurity_criterion(y_i) * y_i.shape[0] / n\n",
    "        return self.impurity_criterion(y) - weighted_child_imp\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        \"\"\"Return the index and value of the feature to split on.\n",
    "\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "\n",
    "        Call the method like this:\n",
    "        >>> index, value, splits = self._choose_split_index(X, y)\n",
    "        >>> X1, y1, X2, y2 = splits\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        index: int\n",
    "            Index of feature\n",
    "        value: int/float/bool/str\n",
    "            Value of feature\n",
    "        splits: tuple\n",
    "            (2d array, 1d array, 2d array, 1d array)\n",
    "        \"\"\"\n",
    "        split_index, split_value, splits = None, None, None\n",
    "        best_gain = 0\n",
    "        for i in range(X.shape[1]):\n",
    "            values = np.unique(X[:, i])\n",
    "            if len(values) <= 1:\n",
    "                continue\n",
    "            for value in values:\n",
    "                X1, y1, X2, y2 = self._make_split(X, y, i, value)\n",
    "                gain = self._information_gain(y, y1, y2)\n",
    "                if gain > best_gain:\n",
    "                    split_index = i\n",
    "                    split_value = value\n",
    "                    splits = (X1, y1, X2, y2)\n",
    "                    best_gain = gain\n",
    "        return split_index, split_value, splits\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return an array of predictions for the feature matrix X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d numpy array\n",
    "            The feature matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y: 1d numpy array\n",
    "            Matrix of predicted labels.\n",
    "        \"\"\"\n",
    "        return np.array([self.root.predict_one(row) for row in X])\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Return string representation of the Decision Tree.\"\"\"\n",
    "        return str(self.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:32:24.403157Z",
     "start_time": "2018-04-11T21:32:24.381025Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-13394491bb0d>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-13394491bb0d>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    print '%26s   %10s   %10s' % (\"FEATURES\", \"ACTUAL\", \"PREDICTED\")\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# run_decision_tree_pruning.py\n",
    "# import pandas as pd\n",
    "# from itertools import izip\n",
    "# from DecisionTreePruning import DecisionTreePruning\n",
    "\n",
    "\n",
    "def test_tree(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    y = df.pop('Result').values\n",
    "    X = df.values\n",
    "    print(X)\n",
    "    \n",
    "    #tree = DecisionTree(leaf_size=10, depth=5, same_ratio=0.8, error_threshold=0.1)\n",
    "    tree = DecisionTreePruning()\n",
    "    tree.fit(X, y, df.columns)\n",
    "    tree.prune(X, y)\n",
    "    print(tree)\n",
    "\n",
    "    y_predict = tree.predict(X)\n",
    "    print '%26s   %10s   %10s' % (\"FEATURES\", \"ACTUAL\", \"PREDICTED\")\n",
    "    print '%26s   %10s   %10s' % (\"----------\", \"----------\", \"----------\")\n",
    "    for features, true, predicted in izip(X, y, y_predict):\n",
    "        print '%26s   %10s   %10s' % (str(features), str(true), str(predicted))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_tree('data/playgolf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T21:09:24.586011Z",
     "start_time": "2018-04-11T21:09:24.566017Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-00e1c2506325>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-00e1c2506325>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    print '%35s   %10s   %10s' % (\"FEATURES\", \"ACTUAL\", \"PREDICTED\")\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# run_decision_tree_regressor.py\n",
    "# import pandas as pd\n",
    "# from itertools import izip\n",
    "# from DecisionTreeRegressor import DecisionTreeRegressor\n",
    "#!/usr/bin/env python2.7\n",
    "\n",
    "def test_tree(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    y = df.pop('Humidity').values\n",
    "    X = df.values\n",
    "    print(X)\n",
    "    \n",
    "    tree = DecisionTreeRegressor()\n",
    "    tree.fit(X, y, df.columns)\n",
    "    tree.prune(X, y)\n",
    "    print(tree)\n",
    "\n",
    "    y_predict = tree.predict(X)\n",
    "    print '%35s   %10s   %10s' % (\"FEATURES\", \"ACTUAL\", \"PREDICTED\")\n",
    "    print '%35s   %10s   %10s' % (\"----------\", \"----------\", \"----------\")\n",
    "    for features, true, predicted in izip(X, y, y_predict):\n",
    "        print '%35s   %10d   %10d' % (str(features), true, predicted)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_tree('data/playgolf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
