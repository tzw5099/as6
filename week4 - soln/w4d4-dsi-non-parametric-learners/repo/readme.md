## k Nearest Neighbors and Decision Trees  

Today we will be playing with non-parametric models. Parametric models (like linear & logistic regression) assume a distribution or generating function that is defined by a set of parameters (e.g., beta coefficients). Then those parameters are varied until the model best fits the data.

Non-parametric models make no such assumptions about what generated the data. Instead, they define some algorithm for finding patterns or associations between the features & target.

We will be using two such machine learning models: **k Nearest Neighbors** and **Decision Trees**.


### Lecture Notes

Notes on [decision trees and kNN](https://github.com/gschool/DSI_Lectures/blob/master/non-parametric-learners/giovanna_thron/lecture.md)

### Assignment

* Individual: [kNN](individual.md)
* Pair: [Decision Trees](pair.md)


### Goals

* Train vs. Test
* Non-parametric models
* CART algorithm
* Conditional Independence
* Maximum Likelihood
* Conditional Probability Table
* Gini impurity, entropy, and information gain


#### Decision Trees

* [Applied Data Science](http://columbia-applied-data-science.github.io/appdatasci.pdf): Chapter 9.4 (p. 100 - p. 104)
* Machine Learning in Action: Chapter 3 ([ID3](http://en.wikipedia.org/wiki/ID3_algorithm))
* [scikit-learn docs: Decision Trees](http://scikit-learn.org/stable/modules/tree.html)

#### k Nearest Neighbors (kNN)
* Machine Learning in Action: Chapter 2
* [scikit-learn docs: K Neighbors Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)


